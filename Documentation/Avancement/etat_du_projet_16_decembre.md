Je pars de **tout ce qu‚Äôon s‚Äôest dit jusqu‚Äôici** + quelques docs officiels (AgileX, RTAB-Map, Cartographer, Zhao 2023‚Ä¶). Voici un **rapport complet** de ton projet √† l‚Äôinstant T.

---

# 1. Contexte g√©n√©ral du projet

* **Plateforme** : robot mobile **AgileX LIMO ROS2**

  * Version ROS2 avec **Intel NUC i7** sous **Ubuntu 22.04 + ROS 2 Humble**.([Agilex Robotics][1])
  * Pens√©e comme plateforme d‚Äôenseignement/recherche en navigation autonome.

* **Capteurs principaux sur ta version**

  * **LiDAR** : EAI **T-mini Pro** 360¬∞ (t√©l√©m√®tre 2D) pour SLAM 2D & √©vitement d‚Äôobstacles.([G√©n√©ration Robots][2])
  * **Cam√©ra RGB-D** : **Orbbec Dabai** pour SLAM visuel 3D (RTAB-Map).([generationrobots.com][3])
  * **IMU** : centrale inertielle (type HI226) pour attitude et odom√©trie inertielle.([Ghostysky][4])
  * **Cam√©ra √† √©v√®nements DAVIS-346** (events + image APS + IMU int√©gr√©e) ajout√©e dans **le cadre du projet**, pas dans la config de base du LIMO.

* **M√©canique et mobilit√©**

  * 4 modes de locomotion : **diff√©rentiel 4 roues, Ackermann, chenilles, Mecanum**.([G√©n√©ration Robots][2])
  * Dimensions ‚âà 322 √ó 220 √ó 251 mm ; masse ‚âà 4,8 kg.([wevolver.com][5])

* **Contexte p√©dagogique / scientifique**

  * Projet de **Master 2 orient√© recherche** (perception & robotique).
  * Objectif √† moyen terme : profil R&D / th√®se en robotique (SLAM, navigation, perception).

---

# 2. Objectif scientifique du projet

## 2.1 Objectif global

> **Am√©liorer la robustesse et les performances de la navigation autonome du LIMO en conditions difficiles**
> en exploitant une cam√©ra √† √©v√®nements DAVIS-346 pour **d√©tecter les obstacles dynamiques** et enrichir la pile **SLAM + Nav2** d√©j√† pr√©sente sur le robot.

Conditions difficiles cibl√©es :

* mouvements rapides du robot,
* sc√®nes **HDR** (fortes variations lumineuses),
* **faible √©clairage**,
* environnements **peu textur√©s**,
* pr√©sence d‚Äô**obstacles dynamiques** (personnes, objets en mouvement).

## 2.2 Axes de travail

Le projet est structur√© en **deux grands axes** :

1. **Axe 1 ‚Äì √âvaluation rigoureuse des piles SLAM + Nav2 existantes sur le LIMO**

   * en particulier :

     * **Cartographer LiDAR + Nav2** (SLAM 2D)([Cartographer ROS][6])
     * **RTAB-Map RGB-D + Nav2** (SLAM visuel 3D)([introlab.github.io][7])
   * avec des **m√©triques quantitatives** (ATE/RPE, taux de r√©ussite de navigation, replanifications, collisions, temps, etc.).

2. **Axe 2 ‚Äì D√©tection d‚Äôobstacles dynamiques bas√©e DAVIS-346 + IMU**

   * √† partir de l‚Äôarticle **Zhao, Li, Lyu, ICRA 2023** *Event-based Real-time Moving Object Detection Based On IMU Ego-motion Compensation*.([Northwestern Polytechnical University][8])
   * int√©gration de cette brique dans **Nav2** via une **couche dynamique de costmap**.

La **comparaison principale** se fera entre :

* **Pipeline A (r√©f√©rence)** : RTAB-Map RGB-D + Nav2, sans DAVIS.
* **Pipeline B (nouveau)** : RTAB-Map + Nav2 + brique **Zhao 2023** bas√©e DAVIS-346.

---

# 3. Piles logicielles existantes sur le LIMO

## 3.1 Environnement logiciel

* OS : **Ubuntu 22.04**.
* Middleware : **ROS 2 Humble**.
* LIMO ROS2 fournit une stack pr√©-int√©gr√©e pour SLAM et navigation (LiDAR + RGB-D + IMU) adapt√©e √† la recherche.([Agilex Robotics][1])

Tu as notamment :

* Paquets bas niveau (type `limo_base`) pour le driver, `/cmd_vel`, `/odom`, `/imu`, `/tf`, etc.
* Paquets de **bringup** (`limo_bringup`) pour lancer les d√©mos :

  * SLAM LiDAR Cartographer,
  * SLAM visuel RTAB-Map,
  * Navigation avec Nav2,
  * D√©mos de suivi de ligne, QR code, etc. (vision classique).

## 3.2 SLAM LiDAR + Nav2 (Cartographer)

* **Cartographer** (Google) : syst√®me de SLAM 2D/3D en temps r√©el, avec int√©gration ROS (cartographer_ros).([Cartographer ROS][6])

* Sur le LIMO, il est utilis√© pour :

  * cartographie 2D √† partir du **LiDAR T-mini Pro**,
  * localisation + fourniture de `/map` ‚Üí Nav2.

* Nav2 utilise alors :

  * une **global costmap** et une **local costmap**,
  * un planificateur global (ex. A*, Dijkstra) + un contr√¥leur local (genre DWB/TEB),
  * pour atteindre des objectifs en √©vitant les obstacles.([docs.nav2.org][9])

## 3.3 SLAM visuel RGB-D + Nav2 (RTAB-Map)

* **RTAB-Map** : approche SLAM **graph-based** pour RGB-D / st√©r√©o / LiDAR, avec d√©tection de boucles par apparence.([introlab.github.io][7])
* **rtabmap_ros** fournit l‚Äôint√©gration ROS 1/ROS 2 ; la branche ROS2 supporte Humble.([GitHub][10])

Sur le LIMO :

* la cam√©ra **Orbbec Dabai** alimente RTAB-Map via un flux RGB-D,
* RTAB-Map produit :

  * une carte 3D (nuages de points),
  * une carte 2D projet√©e,
  * une pose du robot dans `/map`.
* Ces sorties alimentent Nav2 pour la navigation **visuelle**.

**Conclusion** : tu disposes d√©j√† de deux pipelines SLAM + Nav2 fonctionnels, dont le **pipeline visuel RTAB-Map + Nav2** sera la **r√©f√©rence** principale pour √©valuer ton ajout DAVIS.

---

# 4. Probl√©matique scientifique d√©taill√©e

Les limites classiques de la navigation bas√©es sur cam√©ras ¬´ frame-based ¬ª et LiDAR en conditions difficiles :

* **Motion blur** en cas de mouvements rapides ‚Üí perte de features pour SLAM visuel.
* **Faible lumi√®re / HDR** ‚Üí images bruit√©es ou satur√©es.
* **Environnements peu textur√©s** (murs blancs, couloirs uniformes) ‚Üí peu de points d‚Äôint√©r√™t.
* **Obstacles dynamiques** : SLAM et costmap standard ne distinguent pas forc√©ment bien ce qui est **statique** de ce qui est **mobile**, ce qui peut induire des plans sous-optimaux ou des collisions.

Les **cam√©ras √† √©v√®nements** (type DAVIS-346) r√©pondent bien √† ces limitations :

* latence tr√®s faible,
* fonctionnement √† tr√®s grande dynamique,
* focalisation sur les **changements** (mouvements), donc id√©ales pour d√©tecter les objets en mouvement m√™me quand l‚Äôimage classique est floue ou sombre.

Ton projet vise donc √† **exploiter ces avantages pour la navigation** :
non pas en refaisant tout le SLAM, mais en ajoutant une **brique de d√©tection d‚Äôobstacles dynamiques** coupl√©e √† la costmap Nav2.

---

# 5. M√©thode √©v√®nementielle choisie : Zhao et al., ICRA 2023

## 5.1 R√©f√©rence

* C. Zhao, Y. Li, Y. Lyu,
  **‚ÄúEvent-based Real-time Moving Object Detection Based On IMU Ego-motion Compensation‚Äù**,
  Proc. IEEE ICRA 2023, pp. 690‚Äì696.([Northwestern Polytechnical University][8])

## 5.2 Id√©e g√©n√©rale de l‚Äôarticle

But :

> D√©tecter **en temps r√©el** les objets en mouvement vus par une cam√©ra √† √©v√®nements **sur une plateforme mobile** (qui bouge elle aussi), en exploitant l‚ÄôIMU pour compenser le **mouvement propre** (ego-motion).

Contributions principales :

1. **Compensation d‚Äôego-motion par warping non lin√©aire**

   * Utilisation des mesures gyroscopiques de l‚ÄôIMU pour mod√©liser la rotation du capteur entre (t_0) et (t).
   * Application d‚Äôune fonction de **warping non lin√©aire** (g√©om√©trie projective) pour mapper chaque √©v√®nement ((x, y, t)) vers une position compens√©e √† un temps de r√©f√©rence (typiquement la fin de la fen√™tre).
   * R√©sultat : les √©v√®nements du **fond statique** s‚Äôalignent, tandis que ceux des objets mobiles restent ¬´ flous ¬ª/d√©plac√©s.([Semantic Scholar][11])

2. **Segmentation dynamique bas√©e sur count image + time image**

   * Construction d‚Äôune **count image** (C(x, y)) (nombre d‚Äô√©v√©nements par pixel) et d‚Äôune **time image** (T(x, y)) (timestamp moyen).
   * Apr√®s compensation :

     * le fond statique a des timestamps moyens homog√®nes,
     * les objets en mouvement ont des timestamps plus r√©cents ou plus dispers√©s.
   * Un **seuil adaptatif** (fonction de la norme de la vitesse angulaire IMU) s√©pare les pixels dynamiques des statiques.([Semantic Scholar][11])

3. **Clustering des objets dynamiques**

   * Calcul d‚Äôun **pseudo-flot optique** pour les pixels dynamiques.
   * Utilisation d‚Äôun **DBSCAN modifi√©** qui tient compte √† la fois :

     * de la proximit√© spatiale des √©v√®nements,
     * de la similarit√© de vitesse.
   * Chaque cluster correspond √† un **objet dynamique**.

4. **Performance temps r√©el**

   * L‚Äôarticle montre que la m√©thode fonctionne **en temps r√©el** sur des plateformes type NUC avec des cam√©ras DAVIS/DVXplorer, et qu‚Äôelle surpasse d‚Äôautres m√©thodes IMU-aided en qualit√© de compensation et densit√© d‚Äô√©v√®nements utiles.([Semantic Scholar][11])

## 5.3 Adaptation pr√©vue pour ton projet

Dans ton projet, cette m√©thode devient :

1. **DAVIS-346 + IMU (LIMO ou DAVIS)**

   * flux `events` (x, y, t, polarit√©),
   * flux `imu` (œâx, œây, œâz).

2. **Node ROS2 `motion_compensation`**

   * impl√©mentation de la **fonction de warping Zhao 2023**
   * sortie : √©v√©nements compens√©s + count image + time image.

3. **Node ROS2 `dynamic_segmentation`**

   * segmentation binaire statique/dynamique via threshold adaptatif sur `T(x,y)`, √©ventuellement coupl√© √† `C(x,y)` (filtrage de bruit).

4. **Node ROS2 `dynamic_clustering`**

   * clustering (connected components ou DBSCAN) des zones dynamiques ‚Üí listes d‚Äôobjets.

5. **Node ROS2 `davis_costmap_layer` (plugin Nav2)**

   * projette les clusters dynamiques en **zones de co√ªt √©lev√©** dans la costmap 2D, pendant un temps court.

Ce pipeline remplace 0-MMS comme m√©thode principale (0-MMS reste seulement dans l‚Äô√©tat de l‚Äôart).

---

# 6. Architecture ROS 2 cible du syst√®me complet

## 6.1 Vue globale des capteurs & piles

**Capteurs :**

* LiDAR T-mini Pro ‚Üí `/scan`
* Cam√©ra RGB-D Orbbec Dabai ‚Üí `/rgb/image`, `/depth/image`, `/camera_info`
* IMU LIMO ‚Üí `/imu`
* DAVIS-346 ‚Üí `/dvs/events`, `/dvs/imu` (+ √©ventuellement APS)

**Pipes existants :**

1. **SLAM LiDAR (Cartographer)**
   `/scan`, `/imu` ‚Üí Cartographer ‚Üí `/map`, `/odom`, `/tf` ‚Üí Nav2.

2. **SLAM visuel (RTAB-Map)**
   `/rgbd` + odom ‚Üí RTAB-Map ‚Üí `/map`, `/odom`, `/tf` ‚Üí Nav2.

**Nouveau pipe DAVIS :**

3. **D√©tection d‚Äôobstacles dynamiques (Zhao 2023)**

   * `/dvs/events` + `/dvs/imu` ‚Üí `motion_compensation_node`
   * ‚Üí `dynamic_segmentation_node`
   * ‚Üí `dynamic_clustering_node`
   * ‚Üí `/davis_dynamic_obstacles` ‚Üí plugin costmap Nav2 `DavisDynamicLayer`.

## 6.2 Deux pipelines de navigation √† comparer

### Pipeline A ‚Äì Baseline

* RTAB-Map RGB-D fournit `/map` + pose.
* Nav2 avec costmaps standard (LiDAR + obstacles statiques).
* Pas de DAVIS.

### Pipeline B ‚Äì Enrichi avec DAVIS

* RTAB-Map + m√™mes r√©glages.
* Nav2 + **DavisDynamicLayer** qui :

  * lit `/davis_dynamic_obstacles`,
  * gonfle les co√ªts autour des obstacles dynamiques,
  * provoque des d√©viations de trajectoire / ralentissements.

Tu compareras **A vs B** sur les **m√™mes parcours** et conditions.

---

# 7. Protocole d‚Äô√©valuation pr√©vu

## 7.1 Sc√©narios de test

Tu as identifi√© plusieurs sc√©narios :

1. **Parcours ‚Äúnormal‚Äù de r√©f√©rence**

   * bon √©clairage, vitesse mod√©r√©e, peu d‚Äôobstacles dynamiques.

2. **HDR / forte variation de lumi√®re**

   * zones tr√®s √©clair√©es / sombres, passages de lumi√®re directe, etc.

3. **Faible √©clairage**

   * couloir sombre, lumi√®res faibles, o√π la cam√©ra RGB-D se d√©grade.

4. **Environnement peu textur√©**

   * murs uniformes, sol homog√®ne ‚Üí peu de features pour SLAM visuel classique.

5. **Pr√©sence d‚Äôobstacles dynamiques**

   * personnes traversant la trajectoire, obstacles mobiles lents/rapides.

## 7.2 M√©triques SLAM

Sur la base de ton experience M1 (LiDAR-SLAM), tu pr√©vois :

* **ATE** (Absolute Trajectory Error) et **RPE** (Relative Pose Error) via `evo`.
* **Drift** par m√®tre.
* **Pertes de suivi / re-initialisations**.
* **Consommation CPU / RAM** si possible.

## 7.3 M√©triques navigation (Nav2)

* **Taux de r√©ussite** (arrive √† la cible ou non).
* **Temps** pour atteindre l‚Äôobjectif.
* **Longueur de trajectoire**.
* **Nombre de replanifications** (global planner).
* **Collisions / near-miss** (bas√© sur logs / observations).
* **Stabilit√© des vitesses lin√©aires et angulaires** (oscillations, arr√™ts brusques, etc.).

Ces m√©triques seront mesur√©es **pour A et B** sur les m√™mes sc√©narios, afin de mettre en √©vidence l‚Äôapport (ou les limites) de la couche DAVIS.

---

# 8. Ressources logicielles de r√©f√©rence d√©j√† identifi√©es

1. **Code de motion compensation (r√©f√©rence ROS1)**

   * Repo Git (zip) type `Jhonny-Li/Motion-compensation`, impl√©mentant la motion compensation + traitement d‚Äô√©v√®nements pour Zhao 2023.
   * Pas de licence claire ‚Üí **code utilis√© comme r√©f√©rence pour r√©-impl√©menter en ROS 2**, pas pour redistribution directe.

2. **RTAB-Map ROS2**

   * `rtabmap_ros` branch `ros2` avec support Humble.([GitHub][10])

3. **Cartographer ROS**

   * `cartographer_ros` (Apache 2.0) pour SLAM 2D LiDAR.([Cartographer ROS][6])

4. **Nav2 documentation**

   * docs sur SLAM + mapping & costmaps pour ROS2.([docs.nav2.org][9])

---

# 9. √âtat d‚Äôavancement **r√©el** du projet

D‚Äôapr√®s nos √©changes :

### 9.1 Clarifi√© / d√©cid√©

* ‚úÖ **Objectif scientifique** clairement formul√© : am√©liorer la navigation en conditions difficiles via DAVIS + obstacles dynamiques.
* ‚úÖ **Deux axes principaux** d√©finis :

  * √©valuation SLAM + Nav2 existants,
  * d√©veloppement brique DAVIS.
* ‚úÖ **Pile de r√©f√©rence pour la comparaison** :

  * **RTAB-Map RGB-D + Nav2** (pipeline A).
* ‚úÖ **M√©thode √©v√®nementielle finale choisie** :

  * article **Zhao et al. 2023, ICRA** (et non 0-MMS).
* ‚úÖ **Architecture logicielle cible** esquiss√©e :

  * chaines de n≈ìuds ROS2 pour motion compensation, segmentation, clustering, costmap layer.
* ‚úÖ **Protocole de test** conceptualis√© :

  * sc√©narios, m√©triques SLAM et Nav2.

### 9.2 En cours / √† clarifier

* üîÑ **Int√©gration mat√©rielle DAVIS-346 sur le LIMO**

  * montage m√©canique, c√¢blage, alimentation,
  * v√©rification de l‚Äôorientation par rapport au rep√®re du robot.
* üîÑ **Choix et mise en place du driver DAVIS sous ROS 2**

  * driver natif ROS2 ou bridge ROS1‚ÜíROS2.
* üîÑ **D√©finition d√©taill√©e des messages ROS** entre :

  * `motion_compensation_node` ‚Üí `dynamic_segmentation` ‚Üí `dynamic_clustering` ‚Üí `davis_costmap_layer`.

### 9.3 Pas encore commenc√© / √† faire

* ‚è≥ **Calibration compl√®te** :

  * calibration interne DAVIS (focale, distorsion),
  * calibration extrins√®que DAVIS ‚Üî IMU (si IMU LIMO utilis√©e),
  * synchronisation temporelle des capteurs (ROS2 + DAVIS).
* ‚è≥ **Portage / impl√©mentation ROS 2 de Zhao 2023** :

  * re-codage du warping non lin√©aire en C++/Python ROS2,
  * construction count/time images en temps r√©el,
  * segmentation dynamique + clustering (CC ou DBSCAN).
* ‚è≥ **D√©veloppement du plugin costmap Nav2** :

  * impl√©mentation d‚Äôune nouvelle couche `DavisDynamicLayer` (plugin C++ Nav2),
  * param√©trage de la fusion avec les autres couches (obstacles, inflation, etc.).
* ‚è≥ **Campagne d‚Äôexp√©rimentations A vs B** :

  * enregistrement de rosbag2,
  * traitement des r√©sultats (evo, scripts Python),
  * production de graphiques / tableaux.
* ‚è≥ **R√©daction structur√©e du m√©moire / rapport** :

  * √©tat de l‚Äôart (SLAM, event-based vision, Zhao 2023, etc.),
  * m√©thodologie, r√©sultats, discussion, perspectives.

---

# 10. Prochaines √©tapes (prioris√©es)

Pour √™tre concret, les **prochaines actions prioritaires** pourraient √™tre :

1. **Mat√©riel + driver DAVIS (bloquant pour tout le reste)**

   * fixer la DAVIS sur le LIMO,
   * faire fonctionner le driver (afficher les √©v√®nements en live dans RViz / rqt).

2. **Mini-d√©mo de motion compensation seule**

   * adapter / r√©impl√©menter la partie warping IMU + visualisation,
   * v√©rifier qualitativement qu‚Äôun fond statique devient net apr√®s compensation.

3. **Impl√©mentation simple de segmentation dynamique**

   * count + time image, seuil adaptatif,
   * visualisation d‚Äôun masque binaire des zones dynamiques.

4. **Clustering simple + export en obstacles 2D**

   * connected components ou DBSCAN,
   * publication de cercles/bo√Ætes en coordonn√©es robot.

5. **Plugin Nav2 minimal**

   * plugin qui, √† partir des obstacles dynamiques, augmente le co√ªt dans la costmap,
   * test sur un sc√©nario simple avec un obstacle mobile.

6. **D√©but de l‚Äô√©valuation A vs B**

   * un premier sc√©nario simple (couloir, 1 obstacle mobile),
   * quelques runs enregistr√©s,
   * premiers graphiques comparatifs.

---

Si tu veux, je peux maintenant te g√©n√©rer √† partir de ce rapport :

* soit une **version ‚Äúacad√©mique‚Äù courte** (1 page) pour ton encadrante,
* soit un **squelette de chapitre de m√©moire** (plan + sections LaTeX) bas√© sur cette synth√®se.

[1]: https://global.agilex.ai/products/limo-ros2?utm_source=chatgpt.com "LIMO ROS2"
[2]: https://static.generation-robots.com/media/limo-ros-user-manual-en.pdf?utm_source=chatgpt.com "1 LIMO Introduction"
[3]: https://www.generationrobots.com/blog/en/agilex-robotics-lance-deux-nouvelles-versions-de-son-robot-limo-limo-pro-limo-ros2/?srsltid=AfmBOorli3px1nDSo0wioj5PcV34hTSwOMBcsmKYrtPWM_k3KvS8xVIC&utm_source=chatgpt.com "AgileX Robotics launches two new versions of its LIMO robot"
[4]: https://www.ghostysky.com/product/agilex-limo-ros2-intel-nuc/?utm_source=chatgpt.com "Agilex LIMO ROS2 (Intel NUC)"
[5]: https://www.wevolver.com/specs/agilex-limo?utm_source=chatgpt.com "AgileX Limo"
[6]: https://google-cartographer-ros.readthedocs.io/?utm_source=chatgpt.com "Cartographer ROS Integration ‚Äî Cartographer ROS ..."
[7]: https://introlab.github.io/rtabmap/?utm_source=chatgpt.com "RTAB-Map | Real-Time Appearance-Based Mapping"
[8]: https://pure.nwpu.edu.cn/en/publications/event-based-real-time-moving-object-detection-based-on-imu-ego-mo/?utm_source=chatgpt.com "Event-based Real-time Moving Object Detection Based On IMU ..."
[9]: https://docs.nav2.org/setup_guides/sensors/mapping_localization.html?utm_source=chatgpt.com "Mapping and Localization"
[10]: https://github.com/introlab/rtabmap_ros?utm_source=chatgpt.com "introlab/rtabmap_ros: RTAB-Map's ROS package."
[11]: https://www.semanticscholar.org/paper/fa4923ea903111403f13c0f893846a12cd47a02a?utm_source=chatgpt.com "Event-based Real-time Moving Object Detection ..."
